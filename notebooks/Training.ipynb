{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports, paths, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[   INFO   ] MusicExtractorSVM: no classifier models were configured by default\n",
      "/home/btadeusz/miniconda3/envs/ddsp/lib/python3.11/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "from ddsp import DDSP, AudioDataset, CLAPLoss\n",
    "from ddsp.utils import find_checkpoint\n",
    "from ddsp.callbacks import BetaWarmupCallback\n",
    "from ddsp.synths import NoiseBandSynth, SineSynth\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Audio\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import lightning as L\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "torch.set_default_device('cuda')\n",
    "\n",
    "from random import randint\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Model parameters\n",
    "# model_name = 'liget'\n",
    "\n",
    "fs = 44100\n",
    "batch_size = 8\n",
    "n_signal = 1.5 * fs # 1.5 seconds\n",
    "training_path_root = '/home/btadeusz/code/ddsp_vae/training/tarta-relena'\n",
    "models_path_root = '/home/btadeusz/code/ddsp_vae/models/tarta-relena'\n",
    "\n",
    "# model_name = 'tarta-synths'\n",
    "# dataset_path = '/mnt/mariadata/datasets/tarta-relena/synth_noises/processed'\n",
    "\n",
    "model_name = 'tarta_voces_fx_test'\n",
    "dataset_path = '/mnt/mariadata/datasets/tarta-relena/voces_fx/processed'\n",
    "\n",
    "# Training dir\n",
    "training_path = os.path.join(training_path_root, model_name)\n",
    "synth_output_path = os.path.join(models_path_root, f'{model_name}.ts')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = AudioDataset(dataset_path=dataset_path, n_signal=n_signal, sampling_rate=fs)\n",
    "\n",
    "train_set, val_set = random_split(dataset, [0.9, 0.1], generator=torch.Generator(device='cuda'))\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, generator=torch.Generator(device='cuda'))\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=0, generator=torch.Generator(device='cuda'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synth training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDSP parameters\n",
    "latent_size = 4\n",
    "resampling_rate = 32\n",
    "max_freq = 18000\n",
    "perceptual_loss_weight = 0.0\n",
    "\n",
    "# Training config\n",
    "warmup_start = 300\n",
    "warmup_end = 500\n",
    "beta = 0.1\n",
    "max_epochs = 1000\n",
    "learning_rate = 1e-4\n",
    "\n",
    "restart = True\n",
    "\n",
    "if restart:\n",
    "  # Reinitiate the training path\n",
    "  shutil.rmtree(training_path, ignore_errors=True)\n",
    "\n",
    "\n",
    "os.makedirs(training_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialisation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SineSynth.forward() missing 2 required positional arguments: 'parameters' and 'amplitudes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m builders \u001b[38;5;241m=\u001b[39m [sines]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Model\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m ddsp \u001b[38;5;241m=\u001b[39m \u001b[43mDDSP\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m  \u001b[49m\u001b[43msynth_builders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuilders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m  \u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m  \u001b[49m\u001b[43mlatent_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlatent_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m  \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m  \u001b[49m\u001b[43mresampling_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresampling_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m  \u001b[49m\u001b[43mn_melbands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m  \u001b[49m\u001b[43mperceptual_loss_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperceptual_loss_weight\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/code/ddsp_vae/ddsp/ddsp.py:89\u001b[0m, in \u001b[0;36mDDSP.__init__\u001b[0;34m(self, synth_builders, latent_size, fs, encoder_ratios, n_melbands, decoder_ratios, capacity, resampling_factor, learning_rate, kld_weight, perceptual_loss_weight, streaming, device)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresampling_factor \u001b[38;5;241m=\u001b[39m resampling_factor\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# self.synths = synths\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msynths \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msynth_builders\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     90\u001b[0m total_synth_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([s\u001b[38;5;241m.\u001b[39mn_params \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msynths])\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# total_synth_params = 1000\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# self.synths = [NoiseBandSynth(n_filters=1000, fs=fs, resampling_factor=resampling_factor)]\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# ELBO regularization params\u001b[39;00m\n",
      "File \u001b[0;32m~/code/ddsp_vae/ddsp/ddsp.py:89\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresampling_factor \u001b[38;5;241m=\u001b[39m resampling_factor\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# self.synths = synths\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msynths \u001b[38;5;241m=\u001b[39m [\u001b[43mbuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m builder \u001b[38;5;129;01min\u001b[39;00m synth_builders]\n\u001b[1;32m     90\u001b[0m total_synth_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([s\u001b[38;5;241m.\u001b[39mn_params \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msynths])\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# total_synth_params = 1000\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# self.synths = [NoiseBandSynth(n_filters=1000, fs=fs, resampling_factor=resampling_factor)]\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# ELBO regularization params\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ddsp/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ddsp/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: SineSynth.forward() missing 2 required positional arguments: 'parameters' and 'amplitudes'"
     ]
    }
   ],
   "source": [
    "n_filters = 1000\n",
    "n_sines = 500\n",
    "\n",
    "# Synths\n",
    "# nbn = NoiseBandSynth.builder(n_filters=n_filters, fs=fs, resampling_factor=resampling_rate)\n",
    "sines = SineSynth.builder(n_sines=n_sines, fs=fs, resampling_factor=resampling_rate)\n",
    "\n",
    "builders = [sines]\n",
    "\n",
    "# Model\n",
    "ddsp = DDSP(\n",
    "  synth_builders=builders,\n",
    "  fs=fs,\n",
    "  latent_size=latent_size,\n",
    "  learning_rate=learning_rate,\n",
    "  resampling_factor=resampling_rate,\n",
    "  n_melbands=256,\n",
    "  perceptual_loss_weight=perceptual_loss_weight\n",
    ").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/btadeusz/miniconda3/envs/ddsp/lib/python3.11/site-packages/lightning/fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ddsp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m## Start training\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mddsp.device \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mddsp\u001b[49m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     52\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     53\u001b[0m   model\u001b[38;5;241m=\u001b[39mddsp,\n\u001b[1;32m     54\u001b[0m   train_dataloaders\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[1;32m     55\u001b[0m   val_dataloaders\u001b[38;5;241m=\u001b[39mval_loader,\n\u001b[1;32m     56\u001b[0m   ckpt_path\u001b[38;5;241m=\u001b[39mckpt,\n\u001b[1;32m     57\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ddsp' is not defined"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "## Callbacks\n",
    "training_callbacks = []\n",
    "\n",
    "beta_warmup = BetaWarmupCallback(\n",
    "  beta=beta,\n",
    "  start_steps=warmup_start,\n",
    "  end_steps=warmup_end\n",
    ")\n",
    "training_callbacks.append(beta_warmup)\n",
    "\n",
    "# last_checkpoint_callback = ModelCheckpoint(\n",
    "#     filename='last',\n",
    "#     save_top_k=1,  # Save only one file, the most recent one\n",
    "#     save_last=True  # Always save the model at the end of the epoch\n",
    "#   )\n",
    "# training_callbacks.append(last_checkpoint_callback)\n",
    "\n",
    "# best checkopint callback, given the validation loss\n",
    "best_checkpoint_callback = ModelCheckpoint(\n",
    "  filename='best',\n",
    "  monitor='val_loss',\n",
    "  mode='min',\n",
    "  save_top_k=1,  # Save only one file, the best one\n",
    ")\n",
    "training_callbacks.append(best_checkpoint_callback)\n",
    "\n",
    "## Trainer\n",
    "tb_logger = TensorBoardLogger(training_path_root, name=model_name)\n",
    "\n",
    "# from lightning.pytorch.profilers import PyTorchProfiler\n",
    "# profiler = PyTorchProfiler(with_stack=True)\n",
    "\n",
    "trainer = L.Trainer(\n",
    "  callbacks=training_callbacks,\n",
    "  max_epochs=max_epochs,\n",
    "  accelerator='cuda',\n",
    "  precision=16,\n",
    "  log_every_n_steps=0,\n",
    "  logger=tb_logger,\n",
    "  # profiler=profiler\n",
    ")\n",
    "\n",
    "ckpt = find_checkpoint(training_path, return_none=True)\n",
    "if ckpt is not None:\n",
    "  print(f'Restoring from checkpoint {ckpt}')\n",
    "\n",
    "print(\"Fitting\")\n",
    "## Start training\n",
    "print(f'ddsp.device {ddsp.device}')\n",
    "trainer.fit(\n",
    "  model=ddsp,\n",
    "  train_dataloaders=train_loader,\n",
    "  val_dataloaders=val_loader,\n",
    "  ckpt_path=ckpt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ddsp.synths[0]._filterbank.noisebands.device)\n",
    "ddsp.to('cuda').device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddsp = ddsp.cuda()\n",
    "from random import randint\n",
    "test_x = dataset[randint(0, len(dataset))].unsqueeze(0)\n",
    "\n",
    "samples = 44100 * 5\n",
    "start = randint(0, len(dataset._audio) - samples)\n",
    "test_x = dataset._audio[start:start+samples].unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "  test_y = ddsp(test_x)\n",
    "\n",
    "print(\"Input\")\n",
    "display(Audio(test_x.squeeze(0).cpu().numpy(), rate=fs))\n",
    "\n",
    "print(\"Output\")\n",
    "display(Audio(test_y.squeeze().cpu().numpy(), rate=fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning with CLAP loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Freeze parameters\n",
    "# ddsp = ddsp.cuda()\n",
    "# ddsp.train()\n",
    "# # ddsp.encoder.eval()\n",
    "# for param in ddsp.encoder.parameters():\n",
    "#   param.requires_grad = False\n",
    "\n",
    "# ddsp._recons_loss = CLAPLoss()\n",
    "\n",
    "# max_epochs = 100\n",
    "# trainer = L.Trainer(\n",
    "#   callbacks=training_callbacks,\n",
    "#   max_epochs=max_epochs,\n",
    "#   accelerator='cuda',\n",
    "#   precision=16,\n",
    "#   log_every_n_steps=0,\n",
    "#   logger=tb_logger,\n",
    "#   # profiler=profiler\n",
    "# )\n",
    "\n",
    "# trainer.fit(\n",
    "#   model=ddsp,\n",
    "#   train_dataloaders=train_loader,\n",
    "#   val_dataloaders=val_loader,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model\n",
    "!python -m cli.export --model_directory {training_path} --output_path {synth_output_path} --type best\n",
    "\n",
    "print(f'Model saved at {synth_output_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prior training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params & configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_batch_size = 128\n",
    "\n",
    "sequence_length = 64\n",
    "embedding_dim = 32\n",
    "nhead = 8\n",
    "num_layers = 4\n",
    "quantization_channels = 16\n",
    "lr = 1e-4\n",
    "\n",
    "prior_epochs = 10000\n",
    "dataset_stride_factor = 0.05\n",
    "\n",
    "reinitiate = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddsp.prior import Prior, PriorDataset\n",
    "\n",
    "prior_model_name = f'{model_name}-prior'\n",
    "\n",
    "# Training dir\n",
    "prior_training_path = os.path.join(training_path_root, prior_model_name)\n",
    "\n",
    "# Reinitiate the training path\n",
    "if reinitiate:\n",
    "  shutil.rmtree(prior_training_path, ignore_errors=True)\n",
    "  os.makedirs(prior_training_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior dataset\n",
    "The Prior dataset is the same as the audio dataset encoded by the synth encoder into the latent space and arranged into sequences, ready for the sequence prediction training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_dataset = PriorDataset(\n",
    "  audio_dataset_path=dataset_path,\n",
    "  encoding_model_path=synth_output_path,\n",
    "  sequence_length=sequence_length+1,\n",
    "  sampling_rate=fs,\n",
    "  device='cuda',\n",
    "  stride_factor=dataset_stride_factor\n",
    ")\n",
    "normalization_dict = prior_dataset.normalization_dict\n",
    "\n",
    "generator = torch.Generator(device='cuda')\n",
    "prior_train_set, prior_val_set = random_split(prior_dataset, [0.8, 0.2], generator=generator)\n",
    "prior_train_loader = DataLoader(prior_train_set, batch_size=prior_batch_size, shuffle=True, generator=generator)\n",
    "prior_val_loader = DataLoader(prior_val_set, batch_size=prior_batch_size, shuffle=False, generator=generator)\n",
    "\n",
    "latent_size = prior_dataset[0].shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialisation and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_checkpoint = find_checkpoint(prior_training_path)\n",
    "prior = Prior.load_from_checkpoint(prior_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or initialize for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = Prior(\n",
    "  latent_size=latent_size,\n",
    "  embedding_dim=embedding_dim,\n",
    "  quantization_channels=quantization_channels,\n",
    "  max_len=sequence_length,\n",
    "  lr=lr,\n",
    "  nhead=nhead,\n",
    "  num_layers=num_layers,\n",
    "  normalization_dict=normalization_dict\n",
    ")\n",
    "\n",
    "prior_logger = TensorBoardLogger(prior_training_path, name=prior_model_name)\n",
    "\n",
    "prior_callbacks = []\n",
    "prior_checkpoint_callback = ModelCheckpoint(\n",
    "  dirpath=prior_training_path,\n",
    "  filename='best',\n",
    "  monitor='val_loss',\n",
    "  mode='min'\n",
    ")\n",
    "prior_callbacks.append(prior_checkpoint_callback)\n",
    "\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=1000, mode='max', stopping_threshold=0.99)\n",
    "prior_callbacks.append(early_stopping)\n",
    "\n",
    "prior_trainer = L.Trainer(\n",
    "  callbacks=prior_callbacks,\n",
    "  accelerator='cuda',\n",
    "  log_every_n_steps=4,\n",
    "  logger=prior_logger,\n",
    "  max_epochs=prior_epochs\n",
    ")\n",
    "\n",
    "ckpt_path = find_checkpoint(prior_training_path, return_none=True)\n",
    "if ckpt_path is not None:\n",
    "  print(f'Resuming training from checkpoint {ckpt_path}')\n",
    "\n",
    "\n",
    "# Start training\n",
    "prior_trainer.fit(\n",
    "  model=prior,\n",
    "  train_dataloaders=prior_train_loader,\n",
    "  val_dataloaders=prior_val_loader,\n",
    "  ckpt_path=ckpt_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_val_loss = prior_trainer.callback_metrics['val_acc']\n",
    "print(f'Final validation loss: {prior_val_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unconditional generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import jit\n",
    "synth = jit.load(synth_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate sequence\n",
    "prior = prior.cuda()\n",
    "prior.eval()\n",
    "\n",
    "num_steps = 1024\n",
    "prime_len = sequence_length // 4\n",
    "\n",
    "orig_sequence = prior_dataset[randint(0, len(prior_dataset))].cuda()\n",
    "prime = orig_sequence[:prime_len, :]\n",
    "# prime = torch.randn_like(prime)\n",
    "sequence = prior.generate(prime, num_steps, 1.0)\n",
    "latents = sequence.unsqueeze(0)\n",
    "\n",
    "# Decode to audio\n",
    "with torch.no_grad():\n",
    "  # latents, _ = synth.pretrained.encoder.reparametrize(mu, logvar)\n",
    "  audio = synth.decode(latents.permute(0, 2, 1).to('cpu'))\n",
    "\n",
    "# display latents\n",
    "plt.plot(latents.squeeze(0).cpu().numpy()[:,0], label='generated', marker='o')\n",
    "\n",
    "plt.axvline(x=prime_len-1, color='r', linestyle='--')\n",
    "\n",
    "plt.plot(orig_sequence.cpu().numpy()[:,0], linestyle=':', label='original', marker='x')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "# zoom around the prime\n",
    "margin = 64\n",
    "# plt.xlim(prime_len-margin, prime_len+margin)\n",
    "plt.show()\n",
    "\n",
    "# display audio\n",
    "audio = audio.cpu().numpy().squeeze()\n",
    "audio = audio / audio.max()\n",
    "audio_widget = Audio(audio, rate=fs)\n",
    "display(audio_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export prior model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_model_path = os.path.join(models_path_root, f'{prior_model_name}.ts')\n",
    "!python -m cli.export --model_directory {training_path} --prior_directory {prior_training_path} --output_path {prior_model_path} --type best\n",
    "\n",
    "print(f'Model saved at {prior_model_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior.eval()\n",
    "orig_sequence = prior_dataset[randint(0, len(prior_dataset))].cuda()\n",
    "slen = 256\n",
    "\n",
    "x = orig_sequence[:-1]\n",
    "y = prior._quantizer(prior.normalize(orig_sequence[1:]))\n",
    "\n",
    "# loss = prior.training_step(x.unsqueeze(0), _)\n",
    "# print(loss)\n",
    "\n",
    "y_hat = prior(x.unsqueeze(0)).argmax(dim=-1).squeeze(0)\n",
    "\n",
    "x = prior._quantizer(prior.normalize(x))\n",
    "\n",
    "\n",
    "idx = 1\n",
    "\n",
    "print(f'acc: {(y_hat == y).float().sum() / y_hat.numel()}')\n",
    "print(f'y[{idx}]: {y[idx]}')\n",
    "print(f'y_hat[{idx}]: {y_hat[idx]}')\n",
    "\n",
    "print(f'x[{idx+1}]: {x[idx+1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = prior_dataset[randint(0, len(prior_dataset))].cuda().unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "  x = next(iter(prior_train_loader))[:, ...]\n",
    "  print(x.shape)\n",
    "  print(prior._step(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Number of steps to generate\n",
    "num_steps = 10\n",
    "\n",
    "prior = prior.train().to('cuda')\n",
    "prior.eval_mode = True\n",
    "\n",
    "# Initialize the sequence with the given random codes\n",
    "# sequence = torch.zeros(1, sequence_length, latent_size, device='cuda')\n",
    "\n",
    "sequence = prior_dataset[101].unsqueeze(0).to('cuda')\n",
    "norm = prior.normalize(sequence)\n",
    "\n",
    "# Generate latent codes autoregressively\n",
    "for i in range(num_steps):\n",
    "  # Predict the next latent code\n",
    "  with torch.no_grad():\n",
    "    logits = prior(sequence[:, -sequence_length:, :])\n",
    "    next_code = prior.sample(logits=logits, temperature=0.1)\n",
    "\n",
    "  # Append the predicted code to the sequence and shift the sequence to the right\n",
    "  sequence = torch.cat((sequence, next_code[:, -1:, :]), dim=1)\n",
    "\n",
    "mu, logvar = sequence.chunk(2, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  display(Audio(dataset[100].cpu().numpy(), rate=fs))\n",
    "  display(Audio(synth.pretrained(dataset[2].unsqueeze(0).cpu()).squeeze().numpy(), rate=fs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddsp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
