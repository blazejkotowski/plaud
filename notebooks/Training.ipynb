{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports, paths, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddsp import DDSP, AudioDataset\n",
    "from ddsp.utils import find_checkpoint\n",
    "from ddsp.callbacks import BetaWarmupCallback\n",
    "from ddsp.synths import NoiseBandSynth, SineSynth\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Audio\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "import lightning as L\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "torch.set_default_device('cuda')\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Model parameters\n",
    "# model_name = 'liget'\n",
    "\n",
    "# training_path_root = '/home/btadeusz/code/ddsp_vae/training/tarta-relena-new'\n",
    "# models_path_root = '/home/btadeusz/code/ddsp_vae/models/tarta-relena-new'\n",
    "\n",
    "# Voces solos\n",
    "# model_name = 'tarta-voces-solo'\n",
    "# dataset_path = '/mnt/mariadata/datasets/tarta-relena/voces_solo/processed'\n",
    "\n",
    "# Voces fx\n",
    "# model_name = 'tarta-voces-fx'\n",
    "# dataset_path = '/mnt/mariadata/datasets/tarta-relena/voces_fx/processed'\n",
    "\n",
    "# Melodic\n",
    "# model_name = 'tarta-melodic'\n",
    "# dataset_path = '/mnt/mariadata/datasets/tarta-relena/melodic/processed'\n",
    "\n",
    "# Percussvie\n",
    "# model_name = 'drums-1lat'\n",
    "# dataset_path = '/mnt/mariadata/datasets/seven_manifolds/drums'\n",
    "\n",
    "# Lows\n",
    "# model_name = 'tarta-lows'\n",
    "# dataset_path = '/mnt/mariadata/datasets/tarta-relena/lows/processed'\n",
    "\n",
    "# Noisy\n",
    "# model_name = 'tarta-noisy'\n",
    "# dataset_path = '/mnt/mariadata/datasets/tarta-relena/noisy/processed'\n",
    "\n",
    "# Water\n",
    "# model_name = 'tarta-water'\n",
    "# dataset_path = '/mnt/mariadata/datasets/tarta-relena/water/processed'\n",
    "\n",
    "# Tarta Relena\n",
    "# model_name = 'tarta-relena'\n",
    "# dataset_path = '/mnt/mariadata/datasets/tarta-relena/temas/processed'\n",
    "\n",
    "# ICLC\n",
    "\n",
    "# training_path_root = '/home/btadeusz/code/ddsp_vae/training/iclc'\n",
    "# models_path_root = '/home/btadeusz/code/ddsp_vae/models/iclc'\n",
    "\n",
    "# # guitar loops\n",
    "# model_name = 'iclc-guitar-loops'\n",
    "# dataset_path = '/mnt/mariadata/datasets/iclc/guitar-loops/processed'\n",
    "\n",
    "# noise-models\n",
    "training_path_root = '/home/btadeusz/code/ddsp_vae/training/noise-models'\n",
    "models_path_root = '/home/btadeusz/code/ddsp_vae/models/noise-models'\n",
    "\n",
    "# ankersmit\n",
    "# model_name = 'ankersmit-simple'\n",
    "# dataset_path = '/mnt/mariadata/datasets/noise-artists/ankersmit/processed'\n",
    "\n",
    "# klein\n",
    "# model_name = 'klein'\n",
    "# dataset_path = '/mnt/mariadata/datasets/noise-artists/klein/processed'\n",
    "\n",
    "model_name = 'hecker'\n",
    "dataset_path = '/mnt/mariadata/datasets/noise-artists/hecker/processed'\n",
    "\n",
    "# Training dir\n",
    "training_path = os.path.join(training_path_root, model_name)\n",
    "synth_output_path = os.path.join(models_path_root, f'{model_name}.ts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 44100\n",
    "batch_size = 16\n",
    "n_signal = 1.5 * fs # 1.5 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load dataset\n",
    "# dataset = AudioDataset(dataset_path=dataset_path, n_signal=n_signal, sampling_rate=fs)\n",
    "\n",
    "# train_set, val_set = random_split(dataset, [0.9, 0.1], generator=torch.Generator(device='cuda'))\n",
    "# train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, generator=torch.Generator(device='cuda'))\n",
    "# val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=0, generator=torch.Generator(device='cuda'))\n",
    "\n",
    "# Define data augmentations\n",
    "\n",
    "from ddsp.rave_transforms import *\n",
    "\n",
    "# augmentation_pipeline = Compose([\n",
    "#     RandomPitch(n_signal=n_signal, pitch_range=[0.5, 2.0], prob=0.35),\n",
    "#     RandomCompress(prob=0.05),\n",
    "#     RandomGain(gain_range=(-6, 3), prob=0.05),\n",
    "# ])\n",
    "augmentation_pipeline = None\n",
    "\n",
    "# Load dataset\n",
    "dataset = AudioDataset(dataset_path=dataset_path, n_signal=n_signal, sampling_rate=fs, transform_fn=augmentation_pipeline)\n",
    "\n",
    "# Create random indices for validation (20% of total)\n",
    "total_len = len(dataset)\n",
    "val_len = int(0.2 * total_len)\n",
    "indices = torch.randperm(total_len, generator=torch.Generator(device='cuda'))\n",
    "\n",
    "val_indices = indices[:val_len]\n",
    "train_indices = torch.arange(total_len, device='cuda')  # use full dataset for training\n",
    "\n",
    "# Create subset datasets\n",
    "train_set = Subset(dataset, train_indices)\n",
    "val_set = Subset(dataset, val_indices)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, generator=torch.Generator(device='cuda'))\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=0, generator=torch.Generator(device='cuda'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synth training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDSP parameters\n",
    "latent_size = num_params = 4\n",
    "resampling_rate = 32\n",
    "max_freq = 18000\n",
    "perceptual_loss_weight = 0.0\n",
    "\n",
    "# Training config\n",
    "warmup_start = 1000\n",
    "warmup_end = 3000\n",
    "beta = 5.0\n",
    "max_epochs = 10000\n",
    "learning_rate = 1e-4\n",
    "capacity = 64\n",
    "latent_smoothing_kernel = 513\n",
    "decoder_gru_layers = 2\n",
    "\n",
    "restart = False\n",
    "\n",
    "if restart:\n",
    "  # Reinitiate the training path\n",
    "  shutil.rmtree(training_path, ignore_errors=True)\n",
    "\n",
    "\n",
    "os.makedirs(training_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialisation and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = find_checkpoint(training_path, return_none=True)\n",
    "ddsp = DDSP.load_from_checkpoint(ckpt, strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = 64\n",
    "n_sines = 300\n",
    "\n",
    "# Synths\n",
    "nbn = NoiseBandSynth.to_config(n_filters=n_filters, fs=fs, resampling_factor=resampling_rate)\n",
    "sines = SineSynth.to_config(n_sines=n_sines, fs=fs, resampling_factor=resampling_rate)\n",
    "\n",
    "configs = [sines, nbn]\n",
    "# configs = [sines]\n",
    "\n",
    "# Model\n",
    "ddsp = DDSP(\n",
    "  synth_configs=configs,\n",
    "  fs=fs,\n",
    "  latent_size=latent_size,\n",
    "  num_params=num_params,\n",
    "  learning_rate=learning_rate,\n",
    "  resampling_factor=resampling_rate,\n",
    "  n_melbands=128,\n",
    "  perceptual_loss_weight=perceptual_loss_weight,\n",
    "  capacity=capacity,\n",
    "  latent_smoothing_kernel=latent_smoothing_kernel,\n",
    "  decoder_gru_layers=decoder_gru_layers,\n",
    ").to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "## Callbacks\n",
    "training_callbacks = []\n",
    "\n",
    "beta_warmup = BetaWarmupCallback(\n",
    "  beta=beta,\n",
    "  start_steps=warmup_start,\n",
    "  end_steps=warmup_end\n",
    ")\n",
    "training_callbacks.append(beta_warmup)\n",
    "\n",
    "# last_checkpoint_callback = ModelCheckpoint(\n",
    "#     filename='last',\n",
    "#     save_top_k=1,  # Save only one file, the most recent one\n",
    "#     save_last=True  # Always save the model at the end of the epoch\n",
    "#   )\n",
    "# training_callbacks.append(last_checkpoint_callback)\n",
    "\n",
    "# best checkopint callback, given the validation loss\n",
    "best_checkpoint_callback = ModelCheckpoint(\n",
    "  filename='best',\n",
    "  monitor='val_loss',\n",
    "  mode='min',\n",
    "  save_top_k=1,  # Save only one file, the best one\n",
    ")\n",
    "training_callbacks.append(best_checkpoint_callback)\n",
    "\n",
    "## Trainer\n",
    "tb_logger = TensorBoardLogger(training_path_root, name=model_name)\n",
    "\n",
    "# from lightning.pytorch.profilers import PyTorchProfiler\n",
    "# profiler = PyTorchProfiler(with_stack=True)\n",
    "\n",
    "# L.strict_loading\n",
    "trainer = L.Trainer(\n",
    "  callbacks=training_callbacks,\n",
    "  max_epochs=max_epochs,\n",
    "  accelerator='cuda',\n",
    "  precision=16,\n",
    "  log_every_n_steps=0,\n",
    "  logger=tb_logger,\n",
    "  # profiler=profiler\n",
    ")\n",
    "# trainer.lightning_module.strict_loading = False\n",
    "\n",
    "ckpt = find_checkpoint(training_path, return_none=True, typ='best')\n",
    "if ckpt is not None:\n",
    "  print(f'Restoring from checkpoint {ckpt}')\n",
    "\n",
    "# trainer.lightning_module.strict_loading =\n",
    "## Start training\n",
    "ddsp.train()\n",
    "trainer.fit(\n",
    "  model=ddsp,\n",
    "  train_dataloaders=train_loader,\n",
    "  val_dataloaders=val_loader,\n",
    "  ckpt_path=ckpt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddsp = ddsp.cuda()\n",
    "from random import randint\n",
    "test_x = dataset[randint(0, len(dataset))].unsqueeze(0)\n",
    "\n",
    "samples = 44100 * 15\n",
    "start = randint(0, len(dataset._audio) - samples)\n",
    "test_x = dataset._audio[start:start+samples].unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "  mu, scale = ddsp.encoder(test_x)\n",
    "  latents, _ = ddsp.encoder.reparametrize(mu, scale)\n",
    "  latents = ddsp._smooth_latents(latents)\n",
    "  params = test_y = ddsp.decoder(latents)\n",
    "\n",
    "  test_y = ddsp._synthesize(params)\n",
    "\n",
    "  # test_y = ddsp(test_x)\n",
    "\n",
    "print(\"Input\")\n",
    "display(Audio(test_x.squeeze(0).cpu().numpy(), rate=fs))\n",
    "\n",
    "print(\"Latents\")\n",
    "# plot the latents with seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.lineplot(data=latents.squeeze(0).cpu().numpy(), palette='tab10', linewidth=1.0)\n",
    "plt.title('Latents')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Latent value')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"Resynthesis\")\n",
    "display(Audio(test_y.squeeze().cpu().numpy(), rate=fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Generate 3500 latent vectors linearly spaced from -1 to 1 for each latent dimension\n",
    "num_latents = 3500\n",
    "latent_dim = latent_size  # already defined as 16\n",
    "\n",
    "# Create a (1, num_latents, latent_dim) tensor where each latent dimension is swept from -1 to 1\n",
    "latents_grid = torch.linspace(1, -1, num_latents, device='cuda').unsqueeze(1).repeat(1, latent_dim)\n",
    "latents_grid = latents_grid.unsqueeze(0)  # shape: (1, 3500, 16)\n",
    "\n",
    "# Decode to parameters and synthesize audio\n",
    "ddsp.eval()\n",
    "with torch.no_grad():\n",
    "  params = ddsp.decoder(latents_grid)\n",
    "  audio_out = ddsp._synthesize(params)\n",
    "\n",
    "# Listen to the generated audio\n",
    "display(Audio(audio_out.squeeze().cpu().numpy(), rate=fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning with CLAP loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent space analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all latent vectors from the dataset using the trained encoder\n",
    "latents = []\n",
    "ddsp.eval()\n",
    "with torch.no_grad():\n",
    "  for i in range(len(dataset)):\n",
    "    audio = dataset[i].unsqueeze(0).to('cuda')\n",
    "    mu, scale = ddsp.encoder(audio)\n",
    "    cur_latents, _ = ddsp.encoder.reparametrize(mu, scale)\n",
    "    cur_latents = ddsp._smooth_latents(cur_latents)\n",
    "    latents.append(cur_latents)\n",
    "latents = torch.hstack(latents).squeeze(0) # [num_latents, latent_size]\n",
    "\n",
    "# Fit PCA, get the mean and the quantiles\n",
    "ddsp.analyze_latent_space(latents)\n",
    "trainer.save_checkpoint(ckpt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Range Scaling test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_length = 44100  * 5 # 5 second\n",
    "start = randint(0, len(dataset._audio) - chunk_length)\n",
    "audio_chunk = dataset._audio[start:start+chunk_length].unsqueeze(0).to('cuda')\n",
    "print(\"Original audio\")\n",
    "display(Audio(audio_chunk.squeeze(0).cpu().numpy(), rate=fs))\n",
    "\n",
    "ddsp.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "  mu, scale = ddsp.encoder(audio_chunk)\n",
    "  latents, _ = ddsp.encoder.reparametrize(mu, scale)\n",
    "  latents = ddsp._smooth_latents(latents)\n",
    "\n",
    "  params_orig = ddsp.decoder(latents)\n",
    "  audio_orig = ddsp._synthesize(params_orig)\n",
    "  print(\"Original latents range\", latents.min().item(), latents.max().item())\n",
    "  print(\"Original latents audio\")\n",
    "  display(Audio(audio_orig.squeeze(0).cpu().numpy(), rate=fs))\n",
    "\n",
    "\n",
    "  normalized = ddsp.normalize_latents(latents)\n",
    "  print(\"Normalized latents range\", normalized.min().item(), normalized.max().item())\n",
    "  denormalized = ddsp.denormalize_latents(normalized)\n",
    "  params_denormalized = ddsp.decoder(denormalized)\n",
    "  audio_denormalized = ddsp._synthesize(params_denormalized)\n",
    "  print(\"Denormalized latents range\", denormalized.min().item(), denormalized.max().item())\n",
    "  print(\"Denormalized latents audio\")\n",
    "  display(Audio(audio_denormalized.squeeze(0).cpu().numpy(), rate=fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random audio chunk from the dataset\n",
    "chunk_length = 44100  * 5 # 5 second\n",
    "start = randint(0, len(dataset._audio) - chunk_length)\n",
    "audio_chunk = dataset._audio[start:start+chunk_length].unsqueeze(0).to('cuda')\n",
    "\n",
    "# Encode to latents\n",
    "ddsp.eval()\n",
    "with torch.no_grad():\n",
    "  mu, scale = ddsp.encoder(audio_chunk)\n",
    "  latents, _ = ddsp.encoder.reparametrize(mu, scale)\n",
    "  latents = ddsp._smooth_latents(latents)\n",
    "\n",
    "  # Transform latents to params and back\n",
    "  print('latents', latents.shape)\n",
    "  params = ddsp.latents_to_params(latents)\n",
    "  print('params', params.shape)\n",
    "  latents_recon = ddsp.params_to_latents(params)\n",
    "  print('latents_recon', latents_recon.shape)\n",
    "\n",
    "  # Calculate information loss (e.g., mean squared error) between latents and reconstructed latents\n",
    "  info_loss = torch.nn.functional.mse_loss(latents, latents_recon)\n",
    "  print(f\"Information loss (MSE) between latents and reconstructed latents: {info_loss.item():.6f}\")\n",
    "\n",
    "  # Decode audio from original latents\n",
    "  audio_from_latents = ddsp._synthesize(ddsp.decoder(latents))\n",
    "  # Decode audio from reconstructed latents\n",
    "  audio_from_latents_recon = ddsp._synthesize(ddsp.decoder(latents_recon))\n",
    "\n",
    "# Listen to original audio\n",
    "print(\"Original audio chunk\")\n",
    "display(Audio(audio_chunk.squeeze(0).cpu().numpy(), rate=fs))\n",
    "\n",
    "# Listen to audio generated from original latents\n",
    "print(\"Audio from original latents\")\n",
    "display(Audio(audio_from_latents.squeeze().cpu().numpy(), rate=fs))\n",
    "\n",
    "# Listen to audio generated from reconstructed latents\n",
    "print(\"Audio from latents after params transform\")\n",
    "display(Audio(audio_from_latents_recon.squeeze().cpu().numpy(), rate=fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Freeze parameters\n",
    "# ddsp = ddsp.cuda()\n",
    "# ddsp.train()\n",
    "# # ddsp.encoder.eval()\n",
    "# for param in ddsp.encoder.parameters():\n",
    "#   param.requires_grad = False\n",
    "\n",
    "# ddsp._recons_loss = CLAPLoss()\n",
    "\n",
    "# max_epochs = 100\n",
    "# trainer = L.Trainer(\n",
    "#   callbacks=training_callbacks,\n",
    "#   max_epochs=max_epochs,\n",
    "#   accelerator='cuda',\n",
    "#   precision=16,\n",
    "#   log_every_n_steps=0,\n",
    "#   logger=tb_logger,\n",
    "#   # profiler=profiler\n",
    "# )\n",
    "\n",
    "# trainer.fit(\n",
    "#   model=ddsp,\n",
    "#   train_dataloaders=train_loader,\n",
    "#   val_dataloaders=val_loader,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model\n",
    "!python -m cli.export --model_directory {training_path} --output_path {synth_output_path} --type best\n",
    "\n",
    "print(f'Model saved at {synth_output_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prior training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params & configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_batch_size = 128\n",
    "\n",
    "sequence_length = 256\n",
    "embedding_dim = 32\n",
    "nhead = 8\n",
    "num_layers = 4\n",
    "quantization_channels = 16\n",
    "lr = 1e-4\n",
    "\n",
    "prior_epochs = 10000\n",
    "dataset_stride_factor = 0.05\n",
    "\n",
    "reinitiate = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddsp.prior import Prior, PriorDataset\n",
    "\n",
    "prior_model_name = f'{model_name}-prior'\n",
    "\n",
    "# Training dir\n",
    "prior_training_path = os.path.join(training_path_root, prior_model_name)\n",
    "\n",
    "# Reinitiate the training path\n",
    "if reinitiate:\n",
    "  shutil.rmtree(prior_training_path, ignore_errors=True)\n",
    "  os.makedirs(prior_training_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior dataset\n",
    "The Prior dataset is the same as the audio dataset encoded by the synth encoder into the latent space and arranged into sequences, ready for the sequence prediction training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_dataset = PriorDataset(\n",
    "  audio_dataset_path=dataset_path,\n",
    "  encoding_model_path=synth_output_path,\n",
    "  sequence_length=sequence_length+1,\n",
    "  sampling_rate=fs,\n",
    "  device='cuda',\n",
    "  stride_factor=dataset_stride_factor\n",
    ")\n",
    "normalization_dict = prior_dataset.normalization_dict\n",
    "\n",
    "generator = torch.Generator(device='cuda')\n",
    "prior_train_set, prior_val_set = random_split(prior_dataset, [0.8, 0.2], generator=generator)\n",
    "prior_train_loader = DataLoader(prior_train_set, batch_size=prior_batch_size, shuffle=True, generator=generator)\n",
    "prior_val_loader = DataLoader(prior_val_set, batch_size=prior_batch_size, shuffle=False, generator=generator)\n",
    "\n",
    "latent_size = prior_dataset[0].shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialisation and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_checkpoint = find_checkpoint(prior_training_path)\n",
    "prior = Prior.load_from_checkpoint(prior_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or initialize for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = Prior(\n",
    "  latent_size=latent_size,\n",
    "  embedding_dim=embedding_dim,\n",
    "  quantization_channels=quantization_channels,\n",
    "  max_len=sequence_length,\n",
    "  lr=lr,\n",
    "  nhead=nhead,\n",
    "  num_layers=num_layers,\n",
    "  normalization_dict=normalization_dict\n",
    ")\n",
    "\n",
    "prior_logger = TensorBoardLogger(prior_training_path, name=prior_model_name)\n",
    "\n",
    "prior_callbacks = []\n",
    "prior_checkpoint_callback = ModelCheckpoint(\n",
    "  dirpath=prior_training_path,\n",
    "  filename='best',\n",
    "  monitor='val_loss',\n",
    "  mode='min'\n",
    ")\n",
    "prior_callbacks.append(prior_checkpoint_callback)\n",
    "\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=1000, mode='max', stopping_threshold=0.99)\n",
    "prior_callbacks.append(early_stopping)\n",
    "\n",
    "prior_trainer = L.Trainer(\n",
    "  callbacks=prior_callbacks,\n",
    "  accelerator='cuda',\n",
    "  log_every_n_steps=4,\n",
    "  logger=prior_logger,\n",
    "  max_epochs=prior_epochs\n",
    ")\n",
    "\n",
    "ckpt_path = find_checkpoint(prior_training_path, return_none=True)\n",
    "if ckpt_path is not None:\n",
    "  print(f'Resuming training from checkpoint {ckpt_path}')\n",
    "\n",
    "\n",
    "# Start training\n",
    "prior_trainer.fit(\n",
    "  model=prior,\n",
    "  train_dataloaders=prior_train_loader,\n",
    "  val_dataloaders=prior_val_loader,\n",
    "  ckpt_path=ckpt_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_val_loss = prior_trainer.callback_metrics['val_acc']\n",
    "print(f'Final validation loss: {prior_val_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unconditional generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import jit\n",
    "synth = jit.load(synth_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate sequence\n",
    "prior = prior.cuda()\n",
    "prior.eval()\n",
    "\n",
    "num_steps = 10000\n",
    "prime_len = sequence_length // 4\n",
    "\n",
    "orig_sequence = prior_dataset[randint(0, len(prior_dataset))].cuda()\n",
    "prime = orig_sequence[:prime_len, :]\n",
    "# prime = torch.randn_like(prime)\n",
    "print(prime.shape)\n",
    "sequence = prior.generate(prime, num_steps, 1.0)\n",
    "latents = sequence.unsqueeze(0)\n",
    "\n",
    "# Decode to audio\n",
    "with torch.no_grad():\n",
    "  # latents, _ = synth.pretrained.encoder.reparametrize(mu, logvar)\n",
    "  audio = synth.decode(latents.permute(0, 2, 1).to('cpu'))\n",
    "\n",
    "# display latents\n",
    "plt.plot(latents.squeeze(0).cpu().numpy()[:,0], label='generated', marker='o')\n",
    "\n",
    "plt.axvline(x=prime_len-1, color='r', linestyle='--')\n",
    "\n",
    "plt.plot(orig_sequence.cpu().numpy()[:,0], linestyle=':', label='original', marker='x')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "# zoom around the prime\n",
    "margin = 64\n",
    "# plt.xlim(prime_len-margin, prime_len+margin)\n",
    "plt.show()\n",
    "\n",
    "# display audio\n",
    "audio = audio.cpu().numpy().squeeze()\n",
    "audio = audio / audio.max()\n",
    "audio_widget = Audio(audio, rate=fs)\n",
    "display(audio_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export prior model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_model_path = os.path.join(models_path_root, f'{prior_model_name}.ts')\n",
    "!python -m cli.export --model_directory {training_path} --prior_directory {prior_training_path} --output_path {prior_model_path} --type best\n",
    "\n",
    "print(f'Model saved at {prior_model_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior.eval()\n",
    "orig_sequence = prior_dataset[randint(0, len(prior_dataset))].cuda()\n",
    "slen = 256\n",
    "\n",
    "x = orig_sequence[:-1]\n",
    "y = prior._quantizer(prior.normalize(orig_sequence[1:]))\n",
    "\n",
    "# loss = prior.training_step(x.unsqueeze(0), _)\n",
    "# print(loss)\n",
    "\n",
    "y_hat = prior(x.unsqueeze(0)).argmax(dim=-1).squeeze(0)\n",
    "\n",
    "x = prior._quantizer(prior.normalize(x))\n",
    "\n",
    "\n",
    "idx = 1\n",
    "\n",
    "print(f'acc: {(y_hat == y).float().sum() / y_hat.numel()}')\n",
    "print(f'y[{idx}]: {y[idx]}')\n",
    "print(f'y_hat[{idx}]: {y_hat[idx]}')\n",
    "\n",
    "print(f'x[{idx+1}]: {x[idx+1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = prior_dataset[randint(0, len(prior_dataset))].cuda().unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "  x = next(iter(prior_train_loader))[:, ...]\n",
    "  print(x.shape)\n",
    "  print(prior._step(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Number of steps to generate\n",
    "num_steps = 10\n",
    "\n",
    "prior = prior.train().to('cuda')\n",
    "prior.eval_mode = True\n",
    "\n",
    "# Initialize the sequence with the given random codes\n",
    "# sequence = torch.zeros(1, sequence_length, latent_size, device='cuda')\n",
    "\n",
    "sequence = prior_dataset[101].unsqueeze(0).to('cuda')\n",
    "norm = prior.normalize(sequence)\n",
    "\n",
    "# Generate latent codes autoregressively\n",
    "for i in range(num_steps):\n",
    "  # Predict the next latent code\n",
    "  with torch.no_grad():\n",
    "    logits = prior(sequence[:, -sequence_length:, :])\n",
    "    next_code = prior.sample(logits=logits, temperature=0.1)\n",
    "\n",
    "  # Append the predicted code to the sequence and shift the sequence to the right\n",
    "  sequence = torch.cat((sequence, next_code[:, -1:, :]), dim=1)\n",
    "\n",
    "mu, logvar = sequence.chunk(2, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  display(Audio(dataset[100].cpu().numpy(), rate=fs))\n",
    "  display(Audio(synth.pretrained(dataset[2].unsqueeze(0).cpu()).squeeze().numpy(), rate=fs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddsp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
