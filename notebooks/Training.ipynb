{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports, paths, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddsp import DDSP, AudioDataset, CLAPLoss\n",
    "from ddsp.utils import find_checkpoint\n",
    "from ddsp.callbacks import BetaWarmupCallback\n",
    "from ddsp.synths import NoiseBandSynth, SineSynth\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Audio\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import lightning as L\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "torch.set_default_device('cuda')\n",
    "\n",
    "from random import randint\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Model parameters\n",
    "# model_name = 'liget'\n",
    "\n",
    "fs = 44100\n",
    "batch_size = 8\n",
    "n_signal = 1.5 * fs # 1.5 seconds\n",
    "training_path_root = '/home/btadeusz/code/ddsp_vae/training/tarta-relena'\n",
    "models_path_root = '/home/btadeusz/code/ddsp_vae/models/tarta-relena'\n",
    "\n",
    "# model_name = 'tarta-synths'\n",
    "# dataset_path = '/mnt/mariadata/datasets/tarta-relena/synth_noises/processed'\n",
    "\n",
    "model_name = 'tarta_voces_fx_test'\n",
    "dataset_path = '/mnt/mariadata/datasets/tarta-relena/voces_fx/processed'\n",
    "\n",
    "# Training dir\n",
    "training_path = os.path.join(training_path_root, model_name)\n",
    "synth_output_path = os.path.join(models_path_root, f'{model_name}.ts')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = AudioDataset(dataset_path=dataset_path, n_signal=n_signal, sampling_rate=fs)\n",
    "\n",
    "train_set, val_set = random_split(dataset, [0.9, 0.1], generator=torch.Generator(device='cuda'))\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, generator=torch.Generator(device='cuda'))\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=0, generator=torch.Generator(device='cuda'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synth training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDSP parameters\n",
    "latent_size = 4\n",
    "resampling_rate = 32\n",
    "max_freq = 18000\n",
    "perceptual_loss_weight = 0.0\n",
    "\n",
    "# Training config\n",
    "warmup_start = 300\n",
    "warmup_end = 500\n",
    "beta = 0.0\n",
    "max_epochs = 1000\n",
    "learning_rate = 1e-4\n",
    "\n",
    "restart = True\n",
    "\n",
    "if restart:\n",
    "  # Reinitiate the training path\n",
    "  shutil.rmtree(training_path, ignore_errors=True)\n",
    "\n",
    "\n",
    "os.makedirs(training_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialisation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = 500\n",
    "n_sines = 1000\n",
    "\n",
    "# Synths\n",
    "# nbn = NoiseBandSynth.builder(n_filters=n_filters, fs=fs, resampling_factor=resampling_rate)\n",
    "sines = SineSynth.to_config(n_sines=n_sines, fs=fs, resampling_factor=resampling_rate)\n",
    "\n",
    "configs = [sines]\n",
    "\n",
    "# Model\n",
    "ddsp = DDSP(\n",
    "  synth_configs=configs,\n",
    "  fs=fs,\n",
    "  latent_size=latent_size,\n",
    "  learning_rate=learning_rate,\n",
    "  resampling_factor=resampling_rate,\n",
    "  n_melbands=256,\n",
    "  perceptual_loss_weight=perceptual_loss_weight\n",
    ").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "## Callbacks\n",
    "training_callbacks = []\n",
    "\n",
    "beta_warmup = BetaWarmupCallback(\n",
    "  beta=beta,\n",
    "  start_steps=warmup_start,\n",
    "  end_steps=warmup_end\n",
    ")\n",
    "training_callbacks.append(beta_warmup)\n",
    "\n",
    "# last_checkpoint_callback = ModelCheckpoint(\n",
    "#     filename='last',\n",
    "#     save_top_k=1,  # Save only one file, the most recent one\n",
    "#     save_last=True  # Always save the model at the end of the epoch\n",
    "#   )\n",
    "# training_callbacks.append(last_checkpoint_callback)\n",
    "\n",
    "# best checkopint callback, given the validation loss\n",
    "best_checkpoint_callback = ModelCheckpoint(\n",
    "  filename='best',\n",
    "  monitor='val_loss',\n",
    "  mode='min',\n",
    "  save_top_k=1,  # Save only one file, the best one\n",
    ")\n",
    "training_callbacks.append(best_checkpoint_callback)\n",
    "\n",
    "## Trainer\n",
    "tb_logger = TensorBoardLogger(training_path_root, name=model_name)\n",
    "\n",
    "# from lightning.pytorch.profilers import PyTorchProfiler\n",
    "# profiler = PyTorchProfiler(with_stack=True)\n",
    "\n",
    "trainer = L.Trainer(\n",
    "  callbacks=training_callbacks,\n",
    "  max_epochs=max_epochs,\n",
    "  accelerator='cuda',\n",
    "  precision=16,\n",
    "  log_every_n_steps=0,\n",
    "  logger=tb_logger,\n",
    "  # profiler=profiler\n",
    ")\n",
    "\n",
    "ckpt = find_checkpoint(training_path, return_none=True)\n",
    "if ckpt is not None:\n",
    "  print(f'Restoring from checkpoint {ckpt}')\n",
    "\n",
    "## Start training\n",
    "trainer.fit(\n",
    "  model=ddsp,\n",
    "  train_dataloaders=train_loader,\n",
    "  val_dataloaders=val_loader,\n",
    "  ckpt_path=ckpt,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddsp = ddsp.cuda()\n",
    "from random import randint\n",
    "test_x = dataset[randint(0, len(dataset))].unsqueeze(0)\n",
    "\n",
    "samples = 44100 * 5\n",
    "start = randint(0, len(dataset._audio) - samples)\n",
    "test_x = dataset._audio[start:start+samples].unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "  test_y = ddsp(test_x)\n",
    "\n",
    "print(\"Input\")\n",
    "display(Audio(test_x.squeeze(0).cpu().numpy(), rate=fs))\n",
    "\n",
    "print(\"Output\")\n",
    "display(Audio(test_y.squeeze().cpu().numpy(), rate=fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning with CLAP loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Freeze parameters\n",
    "# ddsp = ddsp.cuda()\n",
    "# ddsp.train()\n",
    "# # ddsp.encoder.eval()\n",
    "# for param in ddsp.encoder.parameters():\n",
    "#   param.requires_grad = False\n",
    "\n",
    "# ddsp._recons_loss = CLAPLoss()\n",
    "\n",
    "# max_epochs = 100\n",
    "# trainer = L.Trainer(\n",
    "#   callbacks=training_callbacks,\n",
    "#   max_epochs=max_epochs,\n",
    "#   accelerator='cuda',\n",
    "#   precision=16,\n",
    "#   log_every_n_steps=0,\n",
    "#   logger=tb_logger,\n",
    "#   # profiler=profiler\n",
    "# )\n",
    "\n",
    "# trainer.fit(\n",
    "#   model=ddsp,\n",
    "#   train_dataloaders=train_loader,\n",
    "#   val_dataloaders=val_loader,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model\n",
    "!python -m cli.export --model_directory {training_path} --output_path {synth_output_path} --type best\n",
    "\n",
    "print(f'Model saved at {synth_output_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prior training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params & configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_batch_size = 128\n",
    "\n",
    "sequence_length = 64\n",
    "embedding_dim = 32\n",
    "nhead = 8\n",
    "num_layers = 4\n",
    "quantization_channels = 16\n",
    "lr = 1e-4\n",
    "\n",
    "prior_epochs = 10000\n",
    "dataset_stride_factor = 0.05\n",
    "\n",
    "reinitiate = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddsp.prior import Prior, PriorDataset\n",
    "\n",
    "prior_model_name = f'{model_name}-prior'\n",
    "\n",
    "# Training dir\n",
    "prior_training_path = os.path.join(training_path_root, prior_model_name)\n",
    "\n",
    "# Reinitiate the training path\n",
    "if reinitiate:\n",
    "  shutil.rmtree(prior_training_path, ignore_errors=True)\n",
    "  os.makedirs(prior_training_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior dataset\n",
    "The Prior dataset is the same as the audio dataset encoded by the synth encoder into the latent space and arranged into sequences, ready for the sequence prediction training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_dataset = PriorDataset(\n",
    "  audio_dataset_path=dataset_path,\n",
    "  encoding_model_path=synth_output_path,\n",
    "  sequence_length=sequence_length+1,\n",
    "  sampling_rate=fs,\n",
    "  device='cuda',\n",
    "  stride_factor=dataset_stride_factor\n",
    ")\n",
    "normalization_dict = prior_dataset.normalization_dict\n",
    "\n",
    "generator = torch.Generator(device='cuda')\n",
    "prior_train_set, prior_val_set = random_split(prior_dataset, [0.8, 0.2], generator=generator)\n",
    "prior_train_loader = DataLoader(prior_train_set, batch_size=prior_batch_size, shuffle=True, generator=generator)\n",
    "prior_val_loader = DataLoader(prior_val_set, batch_size=prior_batch_size, shuffle=False, generator=generator)\n",
    "\n",
    "latent_size = prior_dataset[0].shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialisation and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_checkpoint = find_checkpoint(prior_training_path)\n",
    "prior = Prior.load_from_checkpoint(prior_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or initialize for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = Prior(\n",
    "  latent_size=latent_size,\n",
    "  embedding_dim=embedding_dim,\n",
    "  quantization_channels=quantization_channels,\n",
    "  max_len=sequence_length,\n",
    "  lr=lr,\n",
    "  nhead=nhead,\n",
    "  num_layers=num_layers,\n",
    "  normalization_dict=normalization_dict\n",
    ")\n",
    "\n",
    "prior_logger = TensorBoardLogger(prior_training_path, name=prior_model_name)\n",
    "\n",
    "prior_callbacks = []\n",
    "prior_checkpoint_callback = ModelCheckpoint(\n",
    "  dirpath=prior_training_path,\n",
    "  filename='best',\n",
    "  monitor='val_loss',\n",
    "  mode='min'\n",
    ")\n",
    "prior_callbacks.append(prior_checkpoint_callback)\n",
    "\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=1000, mode='max', stopping_threshold=0.99)\n",
    "prior_callbacks.append(early_stopping)\n",
    "\n",
    "prior_trainer = L.Trainer(\n",
    "  callbacks=prior_callbacks,\n",
    "  accelerator='cuda',\n",
    "  log_every_n_steps=4,\n",
    "  logger=prior_logger,\n",
    "  max_epochs=prior_epochs\n",
    ")\n",
    "\n",
    "ckpt_path = find_checkpoint(prior_training_path, return_none=True)\n",
    "if ckpt_path is not None:\n",
    "  print(f'Resuming training from checkpoint {ckpt_path}')\n",
    "\n",
    "\n",
    "# Start training\n",
    "prior_trainer.fit(\n",
    "  model=prior,\n",
    "  train_dataloaders=prior_train_loader,\n",
    "  val_dataloaders=prior_val_loader,\n",
    "  ckpt_path=ckpt_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_val_loss = prior_trainer.callback_metrics['val_acc']\n",
    "print(f'Final validation loss: {prior_val_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unconditional generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import jit\n",
    "synth = jit.load(synth_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate sequence\n",
    "prior = prior.cuda()\n",
    "prior.eval()\n",
    "\n",
    "num_steps = 1024\n",
    "prime_len = sequence_length // 4\n",
    "\n",
    "orig_sequence = prior_dataset[randint(0, len(prior_dataset))].cuda()\n",
    "prime = orig_sequence[:prime_len, :]\n",
    "# prime = torch.randn_like(prime)\n",
    "sequence = prior.generate(prime, num_steps, 1.0)\n",
    "latents = sequence.unsqueeze(0)\n",
    "\n",
    "# Decode to audio\n",
    "with torch.no_grad():\n",
    "  # latents, _ = synth.pretrained.encoder.reparametrize(mu, logvar)\n",
    "  audio = synth.decode(latents.permute(0, 2, 1).to('cpu'))\n",
    "\n",
    "# display latents\n",
    "plt.plot(latents.squeeze(0).cpu().numpy()[:,0], label='generated', marker='o')\n",
    "\n",
    "plt.axvline(x=prime_len-1, color='r', linestyle='--')\n",
    "\n",
    "plt.plot(orig_sequence.cpu().numpy()[:,0], linestyle=':', label='original', marker='x')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "# zoom around the prime\n",
    "margin = 64\n",
    "# plt.xlim(prime_len-margin, prime_len+margin)\n",
    "plt.show()\n",
    "\n",
    "# display audio\n",
    "audio = audio.cpu().numpy().squeeze()\n",
    "audio = audio / audio.max()\n",
    "audio_widget = Audio(audio, rate=fs)\n",
    "display(audio_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export prior model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_model_path = os.path.join(models_path_root, f'{prior_model_name}.ts')\n",
    "!python -m cli.export --model_directory {training_path} --prior_directory {prior_training_path} --output_path {prior_model_path} --type best\n",
    "\n",
    "print(f'Model saved at {prior_model_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior.eval()\n",
    "orig_sequence = prior_dataset[randint(0, len(prior_dataset))].cuda()\n",
    "slen = 256\n",
    "\n",
    "x = orig_sequence[:-1]\n",
    "y = prior._quantizer(prior.normalize(orig_sequence[1:]))\n",
    "\n",
    "# loss = prior.training_step(x.unsqueeze(0), _)\n",
    "# print(loss)\n",
    "\n",
    "y_hat = prior(x.unsqueeze(0)).argmax(dim=-1).squeeze(0)\n",
    "\n",
    "x = prior._quantizer(prior.normalize(x))\n",
    "\n",
    "\n",
    "idx = 1\n",
    "\n",
    "print(f'acc: {(y_hat == y).float().sum() / y_hat.numel()}')\n",
    "print(f'y[{idx}]: {y[idx]}')\n",
    "print(f'y_hat[{idx}]: {y_hat[idx]}')\n",
    "\n",
    "print(f'x[{idx+1}]: {x[idx+1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = prior_dataset[randint(0, len(prior_dataset))].cuda().unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "  x = next(iter(prior_train_loader))[:, ...]\n",
    "  print(x.shape)\n",
    "  print(prior._step(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Number of steps to generate\n",
    "num_steps = 10\n",
    "\n",
    "prior = prior.train().to('cuda')\n",
    "prior.eval_mode = True\n",
    "\n",
    "# Initialize the sequence with the given random codes\n",
    "# sequence = torch.zeros(1, sequence_length, latent_size, device='cuda')\n",
    "\n",
    "sequence = prior_dataset[101].unsqueeze(0).to('cuda')\n",
    "norm = prior.normalize(sequence)\n",
    "\n",
    "# Generate latent codes autoregressively\n",
    "for i in range(num_steps):\n",
    "  # Predict the next latent code\n",
    "  with torch.no_grad():\n",
    "    logits = prior(sequence[:, -sequence_length:, :])\n",
    "    next_code = prior.sample(logits=logits, temperature=0.1)\n",
    "\n",
    "  # Append the predicted code to the sequence and shift the sequence to the right\n",
    "  sequence = torch.cat((sequence, next_code[:, -1:, :]), dim=1)\n",
    "\n",
    "mu, logvar = sequence.chunk(2, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  display(Audio(dataset[100].cpu().numpy(), rate=fs))\n",
    "  display(Audio(synth.pretrained(dataset[2].unsqueeze(0).cpu()).squeeze().numpy(), rate=fs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddsp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
